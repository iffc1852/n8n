<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8" />
  <title>🎙️ ChatGPT Voice 修正版語音助理</title>
  <style>
    body { font-family: "Microsoft JhengHei", sans-serif; text-align: center; margin-top: 40px; }
    input, button { padding: 8px; margin: 5px; width: 260px; font-size: 16px; }
    #status { margin-top: 20px; font-size: 18px; color: #333; }
  </style>
</head>
<body>
  <h2>🤖 ChatGPT Voice 修正版語音助理</h2>
  <input id="aiName" placeholder="輸入喚醒詞 (例如 小文)" /><br>
  <input id="webhookUrl" placeholder="n8n Webhook URL" /><br>
  <button onclick="startWakeListener()">🚀 啟動語音監聽</button>
  <p id="status">尚未啟動</p>

  <script>
    // ===== 全域變數 =====
    let aiName = "";
    let n8nUrl = "";
    let rec;                  // SpeechRecognition 實例
    let isRecognitionActive = false;
    let shouldRecordAfterEnd = false;  // 🔹旗標：是否在辨識結束後啟動錄音
    let mediaRecorder;
    let audioChunks = [];

    // -----------------------------------------------------
    // [1] 啟動喚醒監聽：SpeechRecognition
    // -----------------------------------------------------
    function startWakeListener() {
      aiName = document.getElementById("aiName").value.trim();
      n8nUrl = document.getElementById("webhookUrl").value.trim();
      if (!aiName || !n8nUrl) {
        alert("請輸入喚醒詞與 n8n Webhook URL！");
        return;
      }

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      rec = new SpeechRecognition();
      rec.lang = "zh-TW";
      rec.continuous = true;
      rec.interimResults = false;

      // === 當有語音辨識結果時 ===
      rec.onresult = async (e) => {
        const txt = e.results[e.results.length - 1][0].transcript.trim();
        console.log("🎧 聽到:", txt);
        document.getElementById("status").innerText = "🎧 聽到: " + txt;

        // ✅ 如果包含喚醒詞（AI 名稱）
        if (txt.includes(aiName)) {
          console.log("✅ 偵測到喚醒詞:", aiName);
          document.getElementById("status").innerText = `✅ 喚醒「${aiName}」成功，準備錄音...`;
          shouldRecordAfterEnd = true;  // 🔹等 onend 時再錄音
          rec.abort(); // 🔹不要直接錄音
        }
      };

      // === 發生錯誤時 ===
      rec.onerror = (err) => {
        if (err.error === "aborted") {
          console.log("🎤 辨識中止（準備錄音）");
        } else {
          console.warn("⚠️ 喚醒錯誤:", err.error);
        }
        isRecognitionActive = false;
      };

      // === 辨識完全結束後 ===
      rec.onend = async () => {
        // 🧩 若是因喚醒詞而中止 → 等辨識完全結束再錄音
        if (shouldRecordAfterEnd) {
          shouldRecordAfterEnd = false;
          console.log("🎤 SpeechRecognition 已完全結束，準備開始錄音...");
          await waitMicReady();
          await startMainRecording();
          return;
        }

        // 🧩 若目前沒在錄音 → 自動重啟辨識
        if (!mediaRecorder || mediaRecorder.state !== "recording") {
          console.log("🔁 喚醒監聽重新啟動中...");
          setTimeout(() => rec.start(), 800);
          isRecognitionActive = true;
        } else {
          console.log("🛑 錄音期間禁止自動重啟辨識");
        }
      };

      // 啟動辨識
      rec.start();
      isRecognitionActive = true;
      document.getElementById("status").innerText = "🎤 已啟動喚醒監聽中...";
    }

    // -----------------------------------------------------
    // [2] 等待麥克風釋放
    // -----------------------------------------------------
    async function waitMicReady(maxWait = 2000) {
      console.log("⏳ 等待麥克風釋放中...");
      const start = Date.now();
      while (Date.now() - start < maxWait) {
        try {
          const test = await navigator.mediaDevices.getUserMedia({ audio: true });
          const track = test.getAudioTracks()[0];
          if (track.readyState === "live") {
            track.stop();
            console.log("🎤 麥克風釋放完成");
            return;
          }
        } catch {}
        await new Promise(r => setTimeout(r, 150));
      }
      console.warn("⚠️ 麥克風仍未釋放，強制繼續");
    }

    // -----------------------------------------------------
    // [3] 錄音主流程
    // -----------------------------------------------------
    async function startMainRecording() {
      console.log("🎙️ 開始錄音階段...");
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioChunks = [];
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);

      mediaRecorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: "audio/webm" });
        console.log("📦 錄音完成，大小:", blob.size, "bytes");
        const formData = new FormData();
        formData.append("audio_file", blob, "voice.webm");

        document.getElementById("status").innerText = "📤 傳送錄音中...";
        try {
          const res = await fetch(n8nUrl, { method: "POST", body: formData });
          const text = await res.text();
          console.log("📥 Raw Response:", text);
          if (!text) throw new Error("伺服器無回應");
          const result = JSON.parse(text);
          if (result.success && result.audio) {
            const audio = new Audio("data:audio/mp3;base64," + result.audio);
            audio.play();
            document.getElementById("status").innerText = "🔊 播放 AI 回覆中...";
          } else {
            document.getElementById("status").innerText = "⚠️ 回傳資料格式錯誤";
          }
        } catch (err) {
          console.error("上傳錯誤:", err);
          document.getElementById("status").innerText = "❌ 上傳失敗，請檢查 n8n 回應";
        }

        startWakeListener(); // 錄音結束 → 回到喚醒監聽模式
      };
    
      mediaRecorder.start();
      document.getElementById("status").innerText = "🎙️ 錄音中...(3 秒無聲會自動結束)";

      // 🕐 延遲 1 秒再啟動靜音偵測
      setTimeout(() => {
        console.log("🔊 啟動靜音偵測（延遲1秒）");
        detectSilence(stream);
      }, 1000);
    }


    // -----------------------------------------------------
    // [4] 偵測靜音自動結束錄音
    // -----------------------------------------------------
    function detectSilence(stream) {
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      source.connect(analyser);
      const buffer = new Uint8Array(analyser.fftSize);
      let silenceDuration = 0;

      function loop() {
        analyser.getByteTimeDomainData(buffer);
        let max = Math.max(...buffer.map(v => Math.abs(v - 128)));
        if (max < 10) silenceDuration += 0.1;
        else silenceDuration = 0;

        if (silenceDuration >= 3 && mediaRecorder && mediaRecorder.state === "recording") {
          console.log("🛑 偵測到 3 秒無聲，自動結束錄音");
          mediaRecorder.stop();
          stream.getTracks().forEach(t => t.stop());
          return;
        }
        requestAnimationFrame(loop);
      }
      requestAnimationFrame(loop);
    }
  </script>
</body>
</html>

