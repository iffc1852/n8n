<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8" />
  <title>ğŸ™ï¸ ChatGPT Voice é¢¨æ ¼èªéŸ³åŠ©ç†</title>
  <style>
    body { font-family: "Microsoft JhengHei", sans-serif; text-align: center; margin-top: 40px; }
    input, button { padding: 8px; margin: 5px; width: 260px; font-size: 16px; }
    #status { margin-top: 20px; font-size: 18px; color: #333; }
  </style>
</head>
<body>
  <h2>ğŸ¤– ChatGPT Voice æ¨¡å¼èªéŸ³åŠ©ç†</h2>
  <input id="aiName" placeholder="è¼¸å…¥å–šé†’è© (ä¾‹å¦‚ å°æ–‡)" /><br>
  <input id="webhookUrl" placeholder="n8n Webhook URL" /><br>
  <button onclick="startWakeListener()">ğŸš€ å•Ÿå‹•èªéŸ³ç›£è½</button>
  <p id="status">å°šæœªå•Ÿå‹•</p>

  <script>
    // ===== å…¨åŸŸè®Šæ•¸ =====
    let aiName = "";
    let n8nUrl = "";
    let rec;                // SpeechRecognition å¯¦ä¾‹
    let isRecognitionActive = false;
    let mediaRecorder;
    let audioChunks = [];
    let silenceTimer = null;

    // -----------------------------------------------------
    // [1] å•Ÿå‹•å–šé†’ç›£è½ï¼šä½¿ç”¨ SpeechRecognition
    // -----------------------------------------------------
    function startWakeListener() {
      aiName = document.getElementById("aiName").value.trim();
      n8nUrl = document.getElementById("webhookUrl").value.trim();
      if (!aiName || !n8nUrl) {
        alert("è«‹è¼¸å…¥å–šé†’è©èˆ‡ n8n Webhook URLï¼");
        return;
      }

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      rec = new SpeechRecognition();
      rec.lang = "zh-TW";
      rec.continuous = true;       // æŒçºŒç›£è½
      rec.interimResults = false;

      // ===== ç•¶æœ‰è¾¨è­˜çµæœæ™‚ =====
      rec.onresult = async (e) => {
        const txt = e.results[e.results.length - 1][0].transcript.trim();
        console.log("ğŸ§ è½åˆ°:", txt);
        document.getElementById("status").innerText = "ğŸ§ è½åˆ°: " + txt;

        // âœ… è‹¥åŒ…å«å–šé†’è©ï¼ˆAI åç¨±ï¼‰ï¼Œå‰‡å•Ÿå‹•éŒ„éŸ³
        if (txt.includes(aiName)) {
          console.log("âœ… åµæ¸¬åˆ°å–šé†’è©:", aiName);
          document.getElementById("status").innerText = `âœ… å–šé†’ã€Œ${aiName}ã€æˆåŠŸï¼Œæº–å‚™éŒ„éŸ³...`;
          try { rec.abort(); } catch (err) {}
          isRecognitionActive = false;

          // ğŸ”½ ç­‰å¾…éº¥å…‹é¢¨é‡‹æ”¾ï¼ˆé—œéµï¼‰
          await waitMicReady();

          // ğŸ”½ é–‹å§‹ä¸»éŒ„éŸ³æµç¨‹
          await startMainRecording();
        }
      };

      // ===== è‹¥ç™¼ç”ŸéŒ¯èª¤ =====
      rec.onerror = (err) => {
        if (err.error === "aborted") {
          console.log("ğŸ¤ è¾¨è­˜ä¸­æ­¢ï¼ˆæº–å‚™éŒ„éŸ³ï¼‰");
        } else {
          console.warn("âš ï¸ å–šé†’éŒ¯èª¤:", err.error);
        }
        isRecognitionActive = false;
      };

      // ===== è¾¨è­˜çµæŸå¾Œè‡ªå‹•é‡å•Ÿ =====
      rec.onend = () => {
        if (!isRecognitionActive) {
          console.log("ğŸ” å–šé†’ç›£è½é‡æ–°å•Ÿå‹•ä¸­...");
          setTimeout(() => rec.start(), 800);
          isRecognitionActive = true;
        }
      };

      rec.start();
      isRecognitionActive = true;
      document.getElementById("status").innerText = "ğŸ¤ å·²å•Ÿå‹•å–šé†’ç›£è½ä¸­...";
    }

    // -----------------------------------------------------
    // [2] ç­‰å¾…éº¥å…‹é¢¨é‡‹æ”¾ï¼ˆChrome éœ€è¦ï¼‰
    // -----------------------------------------------------
    async function waitMicReady(maxWait = 2000) {
      console.log("â³ ç­‰å¾…éº¥å…‹é¢¨é‡‹æ”¾ä¸­...");
      const start = Date.now();

      while (Date.now() - start < maxWait) {
        try {
          const test = await navigator.mediaDevices.getUserMedia({ audio: true });
          const track = test.getAudioTracks()[0];
          if (track.readyState === "live") {
            track.stop();
            console.log("ğŸ¤ éº¥å…‹é¢¨é‡‹æ”¾å®Œæˆ");
            return;
          }
        } catch {}
        await new Promise(r => setTimeout(r, 150));
      }
      console.warn("âš ï¸ éº¥å…‹é¢¨ä»æœªé‡‹æ”¾ï¼Œå¼·åˆ¶ç¹¼çºŒ");
    }

    // -----------------------------------------------------
    // [3] ä¸»éŒ„éŸ³æµç¨‹
    // -----------------------------------------------------
    async function startMainRecording() {
      console.log("ğŸ™ï¸ é–‹å§‹éŒ„éŸ³éšæ®µ...");
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioChunks = [];
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);

      mediaRecorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: "audio/webm" });
        console.log("ğŸ“¦ éŒ„éŸ³å®Œæˆï¼Œå¤§å°:", blob.size, "bytes");

        const formData = new FormData();
        formData.append("audio_file", blob, "voice.webm"); // ç¢ºä¿èˆ‡ n8n å°æ‡‰

        document.getElementById("status").innerText = "ğŸ“¤ å‚³é€éŒ„éŸ³ä¸­...";

        try {
          const res = await fetch(n8nUrl, { method: "POST", body: formData });
          const text = await res.text();
          console.log("ğŸ“¥ Raw Response:", text);

          if (!text) throw new Error("ä¼ºæœå™¨ç„¡å›æ‡‰");
          const result = JSON.parse(text);

          if (result.success && result.audio) {
            const audio = new Audio("data:audio/mp3;base64," + result.audio);
            audio.play();
            document.getElementById("status").innerText = "ğŸ”Š æ’­æ”¾ AI å›è¦†ä¸­...";
          } else {
            document.getElementById("status").innerText = "âš ï¸ å›å‚³è³‡æ–™æ ¼å¼éŒ¯èª¤";
          }
        } catch (err) {
          console.error("ä¸Šå‚³éŒ¯èª¤:", err);
          document.getElementById("status").innerText = "âŒ ä¸Šå‚³å¤±æ•—ï¼Œè«‹æª¢æŸ¥ n8n å›æ‡‰";
        }

        // ğŸ” éŒ„éŸ³å®Œç•¢ â†’ å›åˆ°å–šé†’æ¨¡å¼
        startWakeListener();
      };

      // é–‹å§‹éŒ„éŸ³
      mediaRecorder.start();
      document.getElementById("status").innerText = "ğŸ™ï¸ éŒ„éŸ³ä¸­...(3 ç§’ç„¡è²æœƒè‡ªå‹•çµæŸ)";
      detectSilence(stream);
    }

    // -----------------------------------------------------
    // [4] éœéŸ³è‡ªå‹•çµæŸéŒ„éŸ³
    // -----------------------------------------------------
    function detectSilence(stream) {
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      source.connect(analyser);
      const buffer = new Uint8Array(analyser.fftSize);
      let silenceDuration = 0;

      function loop() {
        analyser.getByteTimeDomainData(buffer);
        let max = Math.max(...buffer.map(v => Math.abs(v - 128)));
        if (max < 10) silenceDuration += 0.1;
        else silenceDuration = 0;

        if (silenceDuration >= 3 && mediaRecorder && mediaRecorder.state === "recording") {
          console.log("ğŸ›‘ åµæ¸¬åˆ° 3 ç§’ç„¡è²ï¼Œè‡ªå‹•çµæŸéŒ„éŸ³");
          mediaRecorder.stop();
          stream.getTracks().forEach(t => t.stop());
          return;
        }
        requestAnimationFrame(loop);
      }
      requestAnimationFrame(loop);
    }
  </script>
</body>
</html>
